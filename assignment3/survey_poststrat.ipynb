{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "68aaf480",
   "metadata": {},
   "source": [
    "**TO-DO**\n",
    "- **DONE**: Post stratify on location (Eva)\n",
    "    - Means we have more combinations we need to make predictions for -> 1800 which matches the census\n",
    "    - Also going to change the way we weight by census\n",
    "        - region -> column\n",
    "- **DONE**: Plot regional distribution of census data and compare to our distribution (Eva)\n",
    "    - one for our survey\n",
    "    - one for census\n",
    "    - height of bar is percentage\n",
    "    - one plot two bars per regions (one ours, one census)\n",
    "- **DONE**: Output results to CSV (Eva)\n",
    "    - Unweighted\n",
    "    - Weighted\n",
    "- Write report"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f20f526",
   "metadata": {},
   "source": [
    "We've received some questions about what to do with the post-stratified attitudes obtained in Step 5 of Assignment 3. Please upload a .csv file clearly tabulating your results, along with the code you used. For the report, please give a brief low-dimensional summary or visualization of how weight moved before and after post-stratification, e.g., what census regions gained/lost weight, summing over the finer granularities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4209d1aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "import json\n",
    "import sklearn\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder, LabelEncoder\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6636e828",
   "metadata": {},
   "outputs": [],
   "source": [
    "LogisticRegressionType = sklearn.linear_model._logistic.LogisticRegression\n",
    "LabelEncoderType = sklearn.preprocessing._label.LabelEncoder\n",
    "EncoderType = sklearn.pipeline.Pipeline\n",
    "\n",
    "REGION_COLUMNS = ['New England',\n",
    "                  'Middle Atlantic',\n",
    "                  'East North Central',\n",
    "                  'West North Central',\n",
    "                  'South Atlantic',\n",
    "                  'East South Central',\n",
    "                  'West South Central',\n",
    "                  'Mountain',\n",
    "                  'Pacific']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "86cb3530",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in census data\n",
    "f = open('census_data.json')\n",
    "census_data = json.load(f)\n",
    "census_data = [sublist[:-1] for sublist in census_data]\n",
    "census_df = pd.DataFrame(census_data)\n",
    "census_df = census_df.transpose()\n",
    "column = ['Demographics']\n",
    "column.extend(REGION_COLUMNS)\n",
    "census_df.columns = column\n",
    "f.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "53c88436",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map census categories to survey categories\n",
    "s_to_c_mappings = {\n",
    "    'Gender': {'Male': '1', 'Female': '2'},\n",
    "    'Age': {'> 60': '1', '45-60': '2', '30-44': '3', '18-29': '4'},\n",
    "    'Household Income': {'$150,000+': '1', '$100,000 - $149,999': '2', '$50,000 - $99,999': '3', '$25,000 - $49,999': '4', '$0 - $24,999': '5'},\n",
    "    'Education': {'Graduate degree': '1', 'Bachelor degree': '2', 'Some college or Associate degree': '3', 'High school degree': '4', 'Less than high school degree': '5'}\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "312a09cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine survey data\n",
    "og_survey_df = pd.read_csv('comma-survey.csv', index_col='RespondentID')\n",
    "new_survey_df = pd.read_csv('new_comma_survey.csv', index_col='RespondentID')\n",
    "survey_df = pd.concat([og_survey_df, new_survey_df])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e77ba548",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract question and demographics column names\n",
    "question_cols = list(survey_df.columns)[:7]\n",
    "demographic_cols = list(survey_df.columns)[7:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "bc112f9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows where at least one of the sbustansive questions were not answered\n",
    "survey_df = survey_df.dropna(axis=0, subset=question_cols)\n",
    "# Drop rows where all the demographic questions were unanswered\n",
    "survey_df = survey_df.dropna(axis=0, how='all', subset=demographic_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ed58432",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Encode demographic responses\n",
    "demographics = survey_df[demographic_cols].to_numpy()\n",
    "# Define the ordinal features pipeline\n",
    "enc = Pipeline([\n",
    "    ('imputer', SimpleImputer(strategy='constant', missing_values=np.nan, add_indicator=True)),\n",
    "    ('encoder', OneHotEncoder())\n",
    "])\n",
    "enc.fit(demographics)\n",
    "X_demographics = enc.transform(demographics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "b11b8fe1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit label encoders\n",
    "labelEncoders = [LabelEncoder() for i in range(len(question_cols))]\n",
    "for idx, q in enumerate(question_cols):\n",
    "    y_vals = survey_df[q].to_numpy()\n",
    "    labelEncoders[idx].fit(y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "2c087ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit models\n",
    "models = [LogisticRegression(multi_class='multinomial', max_iter=1000, fit_intercept=False) for i in range(len(question_cols))]\n",
    "for idx, q in enumerate(question_cols):\n",
    "    y_vals = survey_df[q].to_numpy()\n",
    "    Y_vals = labelEncoders[idx].transform(y_vals)\n",
    "    model = models[idx].fit(X_demographics, Y_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "061d2429",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create all demographic combinations, dropping nans\n",
    "demographic_options = []\n",
    "for d in demographic_cols:\n",
    "    demographic_options.append(survey_df[d].unique())\n",
    "unique_combinations = list(itertools.product(*demographic_options))\n",
    "unique_combinations = [i for i in unique_combinations if np.nan not in i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "84b3428b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionary of Questions and Answers\n",
    "qidx_answers = {}\n",
    "for q_idx in range(len(question_cols)):\n",
    "    qidx_answers[q_idx] = list(labelEncoders[q_idx].classes_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "dc826bc3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create dictionaries of running sums\n",
    "unweighted_running_sums = {}\n",
    "weighted_running_sums = {}\n",
    "for q_idx in range(len(question_cols)):\n",
    "    unweighted_running_sums[q_idx] = {}\n",
    "    weighted_running_sums[q_idx] = {}\n",
    "    for a_idx in range(len(qidx_answers[q_idx])):\n",
    "        unweighted_running_sums[q_idx][a_idx] = 0.0\n",
    "        weighted_running_sums[q_idx][a_idx] = 0.0\n",
    "\n",
    "# Calculate the predicted outcome\n",
    "total_population = 0\n",
    "total_combos = len(unique_combinations)\n",
    "for combo in unique_combinations:\n",
    "    # Get the corresponding count from Census data\n",
    "    gender, age, income, education, region = combo\n",
    "    x_vals = enc.transform([combo])\n",
    "    census_mapping = {\n",
    "        'SEX': s_to_c_mappings['Gender'][gender],\n",
    "        'AGEP_RC2': s_to_c_mappings['Age'][age],\n",
    "        'SCHL_RC2': s_to_c_mappings['Household Income'][income],\n",
    "        'HINCP_RC2': s_to_c_mappings['Education'][education]\n",
    "    }\n",
    "    deographic_total = census_df[region].loc[census_df['Demographics'] == census_mapping].values[0]\n",
    "    total_population += deographic_total\n",
    "    # For every substansive question\n",
    "    for q_idx in range(len(question_cols)):\n",
    "        # Get the probability predictions\n",
    "        y_prob = models[q_idx].predict_proba(x_vals)[0]\n",
    "        for a_idx in range(len(y_prob)):\n",
    "            unweighted_running_sums[q_idx][a_idx] += y_prob[a_idx] / total_combos\n",
    "            weighted_running_sums[q_idx][a_idx] += y_prob[a_idx] * deographic_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "acf3cbec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save results to a CSV\n",
    "results = []\n",
    "for q_idx in range(len(question_cols)):\n",
    "    for a_idx in range(len(qidx_answers[q_idx])):\n",
    "        results.append({\n",
    "            'Question': question_cols[q_idx],\n",
    "            'Answer': qidx_answers[q_idx][a_idx],\n",
    "            'Unweighted Probability': round(unweighted_running_sums[q_idx][a_idx], 7),\n",
    "            'Weighted Probability': round(weighted_running_sums[q_idx][a_idx]/ total_population, 7)\n",
    "        })\n",
    "results_df = pd.DataFrame.from_records(results)\n",
    "results_df.to_csv('survey_poststrat_results.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8710250f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q0: In your opinion, which sentence is more gramatically correct?\n",
      "It's important for a person to be honest, kind and loyal.:\t unweighted: 42.87% \tweighted: 44.79%\n",
      "It's important for a person to be honest, kind, and loyal.:\t unweighted: 57.13% \tweighted: 55.21%\n",
      "\n",
      "Q1: Prior to reading about it above, had you heard of the serial (or Oxford) comma?\n",
      "No:\t unweighted: 46.92% \tweighted: 46.61%\n",
      "Yes:\t unweighted: 53.08% \tweighted: 53.39%\n",
      "\n",
      "Q2: How much, if at all, do you care about the use (or lack thereof) of the serial (or Oxford) comma in grammar?\n",
      "A lot:\t unweighted: 28.60% \tweighted: 26.17%\n",
      "Not at all:\t unweighted: 13.20% \tweighted: 12.61%\n",
      "Not much:\t unweighted: 25.16% \tweighted: 25.18%\n",
      "Some:\t unweighted: 33.04% \tweighted: 36.04%\n",
      "\n",
      "Q3: How would you write the following sentence?\n",
      "Some experts say it's important to drink milk, but the data are inconclusive.:\t unweighted: 19.39% \tweighted: 18.69%\n",
      "Some experts say it's important to drink milk, but the data is inconclusive.:\t unweighted: 80.61% \tweighted: 81.31%\n",
      "\n",
      "Q4: When faced with using the word \"data\", have you ever spent time considering if the word was a singular or plural noun?\n",
      "No:\t unweighted: 56.73% \tweighted: 54.66%\n",
      "Yes:\t unweighted: 43.27% \tweighted: 45.34%\n",
      "\n",
      "Q5: How much, if at all, do you care about the debate over the use of the word \"data\" as a singluar or plural noun?\n",
      "A lot:\t unweighted: 10.41% \tweighted: 10.72%\n",
      "Not at all:\t unweighted: 21.18% \tweighted: 19.90%\n",
      "Not much:\t unweighted: 38.30% \tweighted: 39.64%\n",
      "Some:\t unweighted: 30.11% \tweighted: 29.74%\n",
      "\n",
      "Q6: In your opinion, how important or unimportant is proper use of grammar?\n",
      "Neither important nor unimportant (neutral):\t unweighted: 4.96% \tweighted: 3.88%\n",
      "Somewhat important:\t unweighted: 32.15% \tweighted: 31.87%\n",
      "Somewhat unimportant:\t unweighted: 0.91% \tweighted: 0.88%\n",
      "Very important:\t unweighted: 61.48% \tweighted: 62.95%\n",
      "Very unimportant:\t unweighted: 0.50% \tweighted: 0.41%\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tot_pop_for_percent = total_population / 100\n",
    "for q_idx in range(len(question_cols)):\n",
    "    print('Q{}:'.format(q_idx), question_cols[q_idx])\n",
    "    for a_idx in range(len(qidx_answers[q_idx])):\n",
    "        print('{}:\\t unweighted:'.format(qidx_answers[q_idx][a_idx]), '{:,.2%}'.format(unweighted_running_sums[q_idx][a_idx]),\n",
    "             '\\tweighted:', '{:,.2%}'.format(weighted_running_sums[q_idx][a_idx]/ total_population))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a204b26e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot regional distribution of census data and compare to our distribution\n",
    "x_vals = REGION_COLUMNS\n",
    "census_distribution = census_df[REGION_COLUMNS].sum(axis=0).div(total_population).mul(100)\n",
    "census_distribution.name = 'Census Data'\n",
    "our_distribution = survey_df.iloc[:,11].value_counts(normalize=True, sort=False).mul(100)\n",
    "our_distribution.name = 'Survey Data'\n",
    "regional_distribs = pd.concat([census_distribution, our_distribution], axis=1)\n",
    "regional_distribs.plot.bar(title=\"Population Distribution Across Regions\", ylabel=\"Percentage Total Population\", xlabel=\"Region\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97b78736",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
